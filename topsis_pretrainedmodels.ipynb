{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7ff3d9-e883-4c1c-ac32-5865de918e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tf_keras as keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28b64fd-b740-4217-b0fa-4ceac428ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification,AdamW, Trainer, TrainingArguments\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d488d0-6990-4447-b5f9-9250c5338930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.5\n",
      "Recall: 0.6666666666666666\n",
      "F1 Score: 0.5555555555555555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e935560901e45efaeb64f22dbf265f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\datasets--glue. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c26eabaf0a841d18ffa91c3a2d26f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8139038101c142e38c158da799d1528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5219bc7e41bb4c9e9ba1e09604caf862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbb74d0f4fa4232bcf2f0b69d49fc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0676055d96471782c9cf92e6ff7d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efae36e4f44d4444ae9f04aee7cacdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a536a67a00e4441d9078b7847e8fd176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f7837cccf5400b88b9ff6471d7669e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Step 7: Set up training arguments and Trainer\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     62\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     63\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     64\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     65\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     66\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     67\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Evaluate at the end of each epoch\u001b[39;00m\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     70\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     71\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     72\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     }\n\u001b[0;32m     81\u001b[0m )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n",
      "File \u001b[1;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1772\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfget(obj)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2169\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2170\u001b[0m         )\n\u001b[0;32m   2171\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2172\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "\n",
    "# BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # 3 labels: positive, negative, neutral\n",
    "\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "texts = [\"I love this product!\", \"This is the worst experience ever.\", \"It's okay, neither good nor bad.\"]\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    inputs = preprocess_text(texts)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "predictions=predict_sentiment(texts)\n",
    "\n",
    "true_labels=torch.tensor([2, 0, 1]) \n",
    "\n",
    "\n",
    "# Fine-tuning the model with custom data (optional, assumes you have train and eval datasets)\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=lambda p: {\n",
    "        'accuracy': accuracy_score(p.predictions, p.label_ids),\n",
    "        'precision': precision_score(p.predictions, p.label_ids, average='weighted'),\n",
    "        'recall': recall_score(p.predictions, p.label_ids, average='weighted'),\n",
    "        'f1': f1_score(p.predictions, p.label_ids, average='weighted')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Finetuning the model\n",
    "trainer.train()\n",
    "\n",
    "texts=[\"I love this product!\", \"This is the worst experience ever.\", \"It's okay, neither good nor bad.\"]\n",
    "predictions=predict_sentiment(texts)\n",
    "\n",
    "true_labels=torch.tensor([2, 0, 1]) \n",
    "\n",
    "accuracy=accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "precision=precision_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "recall=recall_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "f1=f1_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd0a9c1-0fce-4323-8b26-88114b253d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.1111111111111111\n",
      "Recall: 0.3333333333333333\n",
      "F1 Score: 0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)  # 3 labels: positive, negative, neutral\n",
    "\n",
    "# Step 2: Preprocess input sentences\n",
    "def preprocess_text(texts):\n",
    "    # Tokenizing the text using RoBERTa tokenizer\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "# Example sentences\n",
    "texts = [\"I love this product!\", \"This is the worst experience ever.\", \"It's okay, neither good nor bad.\"]\n",
    "\n",
    "# Step 3: Perform inference on the sentences\n",
    "def predict_sentiment(texts):\n",
    "    inputs = preprocess_text(texts)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "predictions=predict_sentiment(texts)\n",
    "\n",
    "# 0=negative, 1=neutral, 2=positive\n",
    "true_labels=torch.tensor([2, 0, 1])  # Example true labels\n",
    "\n",
    "\n",
    "accuracy=accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "precision=precision_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "recall=recall_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "f1=f1_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ab1165-bbec-421d-bd51-2fe65020074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "Precision: 0.1111111111111111\n",
      "Recall: 0.3333333333333333\n",
      "F1 Score: 0.16666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# BART model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=3)  # 3 labels: positive, negative, neutral\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    # Tokenizing the text using BART tokenizer\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "texts = [\"I love this product!\", \"This is the worst experience ever.\", \"It's okay, neither good nor bad.\"]\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    inputs=preprocess_text(texts)\n",
    "    with torch.no_grad():\n",
    "        outputs=model(**inputs)\n",
    "        logits=outputs.logits\n",
    "        predictions=torch.argmax(logits, dim=-1)\n",
    "    return predictions\n",
    "\n",
    "predictions=predict_sentiment(texts)\n",
    "\n",
    "# 0=negative, 1=neutral, 2=positive\n",
    "true_labels=torch.tensor([2, 0, 1])  # Example true labels\n",
    "\n",
    "\n",
    "accuracy=accuracy_score(true_labels.numpy(), predictions.numpy())\n",
    "precision=precision_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "recall=recall_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "f1=f1_score(true_labels.numpy(), predictions.numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8dc90eb7-76a0-4bc3-9b31-33bf06fe87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [accuracy, precision, recall, f1-score] for each model\n",
    "M = np.array([\n",
    "    [0.667, 0.5, 0.667, 0.556],  # BERT\n",
    "    [0.334, 0.112, 0.334, 0.167],  # RoBERTa\n",
    "    [0.334, 0.167, 0.334, 0.224],  # BART\n",
    "])\n",
    "# Corresponding model names\n",
    "pretrained_models = [\"BERT\", \"RoBERTa\", \"BART\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d11fbb8e-7187-4067-9f52-55317f2041e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Metrics:\n",
      " [[1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.14175258 0.         0.14652956]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data (Euclidean normalization)\n",
    "scaler = MinMaxScaler()\n",
    "M_normalized = scaler.fit_transform(M)\n",
    "\n",
    "print(\"Normalized Metrics:\\n\", M_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0378c8f-f4dd-4380-a112-9d42cd6704c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning equal weights to each metric\n",
    "weights = np.array([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb186afd-8c5a-48af-bb1e-849b54cd6582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideal and Negative Ideal solutions (best and worst values for each metric)\n",
    "ideal_best = np.max(M_normalized, axis=0)  # Maximum values for each metric\n",
    "negative_ideal = np.min(M_normalized, axis=0)  # Minimum values for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a73c50b-a14e-4dcf-a8d6-3862fb0911a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal Best Solution (A+): [1. 1. 1. 1.]\n",
      "Negative Ideal Solution (A-): [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ideal Best Solution (A+):\", ideal_best)\n",
    "print(\"Negative Ideal Solution (A-):\", negative_ideal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ca1f812-8b38-4065-9f1d-bb6d9d84d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance to Ideal Solution (D+): [0.         2.         1.86145116]\n",
      "Distance to Negative Ideal Solution (D-): [2.         0.         0.20387424]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Euclidean distance to Ideal and Negative Ideal\n",
    "dist_ideal = np.linalg.norm(M_normalized - ideal_best, axis=1)\n",
    "dist_negative_ideal = np.linalg.norm(M_normalized - negative_ideal, axis=1)\n",
    "\n",
    "print(\"Distance to Ideal Solution (D+):\", dist_ideal)\n",
    "print(\"Distance to Negative Ideal Solution (D-):\", dist_negative_ideal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cb4b2f5-fcce-4ab2-8d65-d5f94551b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Scores: [1.         0.         0.09871289]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the relative closeness to the ideal solution\n",
    "performance_score = dist_negative_ideal / (dist_ideal + dist_negative_ideal)\n",
    "\n",
    "print(\"Performance Scores:\", performance_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59bc183b-656f-4d05-8bba-9892c5ec0154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Models (from best to worst): ['BERT', 'BART', 'RoBERTa']\n"
     ]
    }
   ],
   "source": [
    "# Rank models based on performance score (higher score = better model)\n",
    "ranked_pretrained_models = np.argsort(performance_score)[::-1]  # Sort in descending order\n",
    "print(\"Ranked Models (from best to worst):\", [pretrained_models[i] for i in ranked_pretrained_models])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7f3ebb0-912b-4d49-ab22-529ec7d0d702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the performance scores (Higher scores = better models)\n",
    "plt.bar(pretrained_models, performance_score, color='skyblue')\n",
    "plt.xlabel('Pre-trainedModels')\n",
    "plt.ylabel('Performance Score')\n",
    "plt.title('TOPSIS Model Ranking')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94fd41-885b-41d0-89c7-efed5c9fd847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d27c01-b82a-4691-be0d-7c4af06992fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916111da-69d9-4101-884b-63ce595e66d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
